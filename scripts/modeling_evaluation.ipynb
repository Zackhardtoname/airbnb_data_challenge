{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Modeling\n",
    "\n",
    "I chose Gradient boost regression tree (GBRT) for its high level of interpretability and relative ease of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8f6203d18806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zack\\documents\\github\\airbnb_dataset_challenge\\env\\lib\\site-packages\\statsmodels\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd_constant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOLS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGLS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWLS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGLSAR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_ls\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRecursiveLS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zack\\documents\\github\\airbnb_dataset_challenge\\env\\lib\\site-packages\\statsmodels\\regression\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myule_walker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zack\\documents\\github\\airbnb_dataset_challenge\\env\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m                                           \u001b[0mcache_readonly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                                           cache_writable)\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memplike\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melregress\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_ELRegOpts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zack\\documents\\github\\airbnb_dataset_challenge\\env\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msm_exceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mValueWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mHessianInversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformula\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhandle_formula_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_matrix_rank\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zack\\documents\\github\\airbnb_dataset_challenge\\env\\lib\\site-packages\\statsmodels\\formula\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mformulatools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhandle_formula_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\zack\\documents\\github\\airbnb_dataset_challenge\\env\\lib\\site-packages\\statsmodels\\formula\\formulatools.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterkeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpatsy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdmatrices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNAAction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'patsy'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'patsy'",
     "output_type": "error"
    }
   ],
   "source": [
    "import pickle\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import HTML\n",
    "from scipy import sparse\n",
    "from sklearn import ensemble\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (GridSearchCV, ShuffleSplit,\n",
    "                                     learning_curve, train_test_split)\n",
    "\n",
    "import helpers\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# runtime parameter\n",
    "grid_search = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Part of the rationale behind the train test split is inspired by this post: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "X = pd.DataFrame(sparse.load_npz(\"../data/X.npz\").toarray())\n",
    "y = pd.read_csv(\"../data/y.csv\", index_col=0, header=None)\n",
    "var_names = pickle.load(open('../data/var_names.pkl', 'rb'))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "for dataset, name in [(X_train, \"X_train\"), (X_test, \"X_test\"), (y_train, \"y_train\"), (y_test, \"y_test\")]:\n",
    "        dataset.to_csv(\"../data/train_test_data/\" + name + \".csv\")\n",
    "\n",
    "y_train = y_train.values.reshape(-1,)\n",
    "y_test = y_test.values.reshape(-1,)\n",
    "\n",
    "params = {'n_estimators': [500], 'max_depth': range(4, 15, 4), 'min_samples_split': range(2, 1003, 400), \n",
    "          'min_samples_leaf':range(30,71,20), 'max_features': [\"sqrt\", \"log2\", None], 'subsample':[0.7,0.75,0.8,0.85],\n",
    "            }\n",
    "\n",
    "rgs = ensemble.GradientBoostingRegressor(learning_rate=0.01, loss='ls', warm_start=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: the following cell could take a while (around 2 hours in my case) to run\n",
    "I could search for the best parameters in groups, but here I chose to search all together to keep the code short and straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if grid_search:\n",
    "    g_search = GridSearchCV(estimator=rgs, param_grid=params, scoring='neg_mean_squared_error', n_jobs=-2, iid=False, cv=3, verbose=10)\n",
    "    g_search.fit(X_train, y_train)\n",
    "    joblib.dump(g_search, open('../models/g_search.pkl', 'wb'))\n",
    "else:\n",
    "    g_search = joblib.load(open('../models/g_search.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(g_search.best_params_)\n",
    "print(g_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rgs = g_search.best_estimator_\n",
    "rgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, rgs.predict(X_test))\n",
    "print(\"Final MSE: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training deviance\n",
    "\n",
    "n_estimators = g_search.best_params_['n_estimators']\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(rgs.staged_predict(X_test)):\n",
    "    test_score[i] = rgs.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(60, 60))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(n_estimators) + 1, rgs.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(n_estimators) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "\n",
    "# Plot feature importance\n",
    "\n",
    "feature_importance = rgs.feature_importances_\n",
    "# make importance relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "ranked_features_asc = np.array(var_names)[sorted_idx]\n",
    "plt.yticks(pos, ranked_features_asc)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.savefig('../imgs/deviance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [the saved plot](../imgs/deviance.png) to see the actual texts.\n",
    "\n",
    "We can see that as we increase the number of estimators for the tree, the errors decrease. \n",
    "Thus, we would increase the number of estimators for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also take a look at the learning curve to examine the bias and variance of our model.\n",
    "\n",
    "Warning: the cell below could take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "params = g_search.best_params_\n",
    "params[\"learning_rate\"] = .01\n",
    "params[\"loss\"] = 'ls'\n",
    "params[\"warm_start\"] = True\n",
    "\n",
    "train_scores_mean = helpers.plot_learning_curve(ensemble.GradientBoostingRegressor(**params), \"Learning Curve\", X_train, y_train, n_jobs=-1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the learning curve graph, there is a large gap between the curves for the training and cross-validation scores. \n",
    "That means there is a high level of variance, and getting more data and/or more training would help lower the error further.\n",
    "\n",
    "For the scalability graph, the fit time goes up linearly with the number of training examples, indicating high scalability of the GBRT model.\n",
    "\n",
    "For the performance of the model, we do see a reasonable diminishing rate of return, as in many other types of models, it is what I would expect.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can further classify the variables by their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ranked_features_dsc = ranked_features_asc[::-1]\n",
    "print(\"Features of most importance (descending):\\n\")\n",
    "# rank the features in descending order\n",
    "higher_cutoff_idx = np.argmax(feature_importance[sorted_idx]>=5)\n",
    "lower_cutoff_idx = np.argmax(feature_importance[sorted_idx]>=1)\n",
    "\n",
    "second_level_idx = (len(feature_importance) - higher_cutoff_idx + 1) * [\"More Important (>=5)\"] + (higher_cutoff_idx - lower_cutoff_idx) * [\"Less Important (1<= && <5)\"] + \\\n",
    "                     (lower_cutoff_idx - 1) * [\"Not Important (<1)\"]\n",
    "feature_df = pd.DataFrame(feature_importance[sorted_idx][::-1], index=[second_level_idx, ranked_features_dsc], columns=[\"Importance\"])\n",
    "\n",
    "HTML(feature_df.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conclusion on variables\n",
    "\n",
    "Most of the numerical variables mentioned in the hypothesis are classified as \"more importance\" or \"less important.\" \n",
    "Note that many transformed and engineered numerical variables such as \"log_bedrooms_per_accommodates\" turn out to be of \"more importance.\" \n",
    "\n",
    "On the other hand, only a few categorical variables tend out to be \"more important\" or \"less important.\" \n",
    "\n",
    "Variables that are important but were not in the hypothesis are \"response_time\" and \"property_type.\" \n",
    "\n",
    "We could take a closer look at the variables that we missed out by looking back at their distributions. We would exam \"minimum_nights\" and \"maximum_nights\" here as an example.\n",
    "\n",
    "Overall, the part about important variables in the hypothesis is mainly correct for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/listings.csv\", converters={\"host_verifications\": literal_eval})\n",
    "df[\"minimum_nights\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "results = sm.OLS(y_pred,sm.add_constant(y_test)).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion on R-squared\n",
    "\n",
    "The R-squared value is .565, meaning the variables used in training could explain 56.5% of the variation in the log price. \n",
    "P values for both the intercept and the slope for are 0, meaning they are statistically significant. \n",
    "It is slightly below the value in our hypothesis, but still relatively satisfactory. \n",
    "\n",
    "For the fitted line, the slope (x1 in the table) is .565, which is not great since ideally, we would like a slope of 1. Yet .565 is as expected for us \n",
    "since it should be close to the R-squared value (for some intuition, read this [post](https://stats.stackexchange.com/questions/87963/does-the-slope-of-a-regression-between-observed-and-predicted-values-always-equa)).\n",
    "The intercept (called \"constant\" in the table) is 2, suggesting when the real price approaches 0, the predicted price would almost be four times as much as the real price \n",
    "(of course, that is an extremely unlikely or unrealistic outlier). In part, the poor intercept value results from the poor slope value. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a scatter plot of our predictions against the true prices in the test dataset and draw a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sns_plot = sns.lmplot(x=\"True Data\", y='Predicted Data', data=pd.DataFrame(list(zip(y_test, y_pred)), columns =['True Data', 'Predicted Data']), fit_reg=True)\n",
    "fig = sns_plot.fig\n",
    "fig.suptitle('True VS Predicted log_prices', fontsize=8)\n",
    "fig.savefig('../imgs/true_vs_predicted_log_prices.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The plot looks great with residual relatively evenly distributed around the fitted line, with a very limited number of outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvement\n",
    "1. I could use auto-transformers (https://datamadness.github.io/Skewness_Auto_Transform) for feature transformation as log transformation is not the only choice.\n",
    "\n",
    "2. Some variables are not used in this challenge. Most notably, the text-based variables such as \"description,\" \"neighborhood_overview,\" and \"transit\" would likely have a significant amount of impact on the prices.\n",
    "A direction for future improvement could be to convert them into numerical variables by taking the average pooling of the word vectors.\n",
    "\n",
    "3. I could also try different models other than GBRT.\n",
    "\n",
    "4. I could use bayesian optimization instead of grid search to save time in finding the best parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}